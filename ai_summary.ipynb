{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c2242b",
   "metadata": {},
   "source": [
    "# Riassunto AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b8762",
   "metadata": {},
   "source": [
    "## Agenti intelligenti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa27a34",
   "metadata": {},
   "source": [
    "### Agenti razionali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df7b33",
   "metadata": {},
   "source": [
    "**Agente**: un sistema che percepisce il suo ambiente attraverso dei **sensori** e agisce su di esso mediante degli **attuatori**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83592b3",
   "metadata": {},
   "source": [
    "**Percezione**: input percettivi in un dato istante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d9b04",
   "metadata": {},
   "source": [
    "**Sequence percettiva**: la storia completa di tutto quello che l'agente ha percepito nella sua esistenza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510b74b",
   "metadata": {},
   "source": [
    "**Funzione agente**: funzione matematica astratta che descrive il comportamento di un agente (descrive la corrispondenza tra una qualsiasi sequenza percettiva e una specifica azione)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015c745",
   "metadata": {},
   "source": [
    "**Programma agente**: implementazione concreta della funzione agente, prende in input la percezione corrente dei sensori e restituisce un'azione agli attuatori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae36c5",
   "metadata": {},
   "source": [
    "**Autonomia**: quando un agente NON si basa sulla conoscenza pregressa inserita dal programmatore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d985b11",
   "metadata": {},
   "source": [
    "**Misure di prestazione**: valutano una sequenza di stati dell'ambiente. Vanno progettate in base all'effetto che si desidera ottenere sull'ambiente invece di su come si pensa che debba comportarsi l'ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff26bd5e",
   "metadata": {},
   "source": [
    "In un dato istante, ciò che è razionale dipende da:\n",
    "- la misura di prestazione che definisce il criterio di successo\n",
    "- la conoscenza pregressa dell'ambiente da parte dell'agente\n",
    "- le azioni che l'agente può effettuare\n",
    "- la sequenza percettiva dell'agente fino all'istante corrente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b61e6",
   "metadata": {},
   "source": [
    "**Agente razionale**: per ogni possibile sequenza di percezioni, sceglie un'azione che massimizzi il valore atteso della sua misura di prestazione, date le informazioni fornite dalla sequenza percettiva e da ogni ulteriore conoscenza dell'agente.\n",
    "\n",
    "Non si limita a raccogliere informazioni, ma deve anche essere in grado di apprendere il più possibile sulla base delle proprie percezioni per compensare la sua iniziale conoscenza parziale o erronea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2b524",
   "metadata": {},
   "source": [
    "![image.png](images/agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27c441",
   "metadata": {},
   "source": [
    "### La struttura degli agenti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f2d14",
   "metadata": {},
   "source": [
    "#### Agenti reattivi semplici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db1abc",
   "metadata": {},
   "source": [
    "- Scelgono le azioni sulla base della percezione corrente, ignorando tutta la storia percettiva\n",
    "- sono basati su regole condizione-azione (**if** condizione **then** azione)\n",
    "- l'**ambiente** deve essere **completamente osservabile**, anche una minima parte di non-osservabilità può causare grandi problemi\n",
    "- spesso non sono in grado di evitare cicli infiniti quando operano in ambienti parzialmente osservabili, questo si può risolvere randomizzando talvolta le azioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825822f9",
   "metadata": {},
   "source": [
    "![image.png](images/agent_reactive_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eefb57",
   "metadata": {},
   "source": [
    "#### Agenti reattivi basati su modello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccea79",
   "metadata": {},
   "source": [
    "- per gestire l'osservabilità parziale si può tener traccia della parte del mondo che non si può vedere nell'istante corrente\n",
    "- quindi l'agente mantiene uno **stato interno** che dipende dalla storia delle percezioni\n",
    "- possiede 2 tipi di conoscenza:\n",
    "    - informazioni sull'evoluzione del mondo indipendentemente dalle sue azioni\n",
    "    - informazioni sull'effetto delle sue azioni sul mondo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239d8ca",
   "metadata": {},
   "source": [
    "![image.png](images/agent_reactive_model_based.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a71fe7",
   "metadata": {},
   "source": [
    "#### Agenti basati su obiettivi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e50955",
   "metadata": {},
   "source": [
    "- oltre a tener traccia dello stato dell'ambiente, memorizza un insieme di obiettivi e sceglie l'azione che lo porterà (prima o poi) a soddisfarli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d984d0",
   "metadata": {},
   "source": [
    "![image.png](images/agent_objective_based.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fde382",
   "metadata": {},
   "source": [
    "#### Agenti basati sull'utilità"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ca7db",
   "metadata": {},
   "source": [
    "- **funzione di utilità**: è un internazionalizzazione della misura di prestazione\n",
    "- sceglie l'azione che massimizza l'utilità attesa dei risultati, ovvero l'utilità che l'agente si attende di ottenere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc850b",
   "metadata": {},
   "source": [
    "![image.png](images/agent_utility_based.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf238246",
   "metadata": {},
   "source": [
    "#### Agenti capaci di apprendere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280ad02",
   "metadata": {},
   "source": [
    "- diviso in 4 componenti:\n",
    "    - **elemento esecutivo**: si occupa della selezione delle azioni esterne, equivale agli agenti descritti precedentemente\n",
    "    - **elemento critico**: dice all'elemento di apprendimento come si sta comportando l'agente rispetto ad uno standard di prestazione prefissato. E' necessario perchè di per se le percezioni non forniscono alcuna indicazione del successo dell'agente\n",
    "    - **elemento di apprendimento**: responsabile del miglioramento interno, utilizza le informazioni provenienti dall'elemento critico riguardo le prestazioni correnti dell'agente e determina se e come modificare l'elemento esecutivo affinchè in futuro si comporti meglio. Può modificare uno qualsiasi dei componenti \"di conoscenza\" mostrati nei diagrammi precedenti\n",
    "    - **generatore di problemi**: suggerisce le azioni che portino ad esperienze nuove e significative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ff367",
   "metadata": {},
   "source": [
    "![image.png](images/agent_capable_learn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb307d44",
   "metadata": {},
   "source": [
    "### Ambienti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0fb76",
   "metadata": {},
   "source": [
    "- gli ambienti sono i problemi di cui gli agenti razionali rappresentano le soluzioni\n",
    "- definire i **PEAS** (Performance, Environment, Actuators, Sensors) per ogni tipo di agente razionale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78520a1e",
   "metadata": {},
   "source": [
    "![image.png](images/environments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2077514",
   "metadata": {},
   "source": [
    "- proprietà degli ambienti:\n",
    "    - **completamente osservabile vs. parzialmente osservabile**: **completamente osservabile** quando i sensori danno accesso allo stato completo dell'ambiente in ogni momento, o almeno che misurino gli aspetti rilevanti. **Parzialmente osservabile** se i sensori sono inaccurati, presenza di rumore o alcuni dati rilevanti non vengono catturati da essi\n",
    "    - **agente singolo vs. multiagente**: **agente singolo**, **multiagente competitivo** o **multiagente cooperativo**\n",
    "    - **deterministico vs. stocastico**: **deterministico** se lo stato successivo dell'ambiente è completamente determinato dallo stato corrente e dall'azione eseguita dall'agente. Altrimenti è **stocastico**\n",
    "    - **episodico vs. sequenziale**: **episodico** quando l'esperienza dell'agente è divisa in episodi atomici, ogni episodio non dipende dalle azioni intraprese in quelli precedenti. **Sequenziale** quando ogni decisione può influenzare tutte quelle successive\n",
    "    - **statico vs. dinamico**: **dinamico** se l'ambiente può cambiare mentre un agente sta pensando altrimenti è **statico**. **Semidinamico** se l'ambiente non cambia con il passare del tempo, ma la valutazione dell'agente si\n",
    "    - **discreto vs. continuo**: ci si riferisce al modo in cui è gestito il tempo\n",
    "    - **noto vs. ignoto**: ci si riferisce alla conoscenza che ha l'agente circa le \"leggi fisiche\" dell'ambiente. In un ambiente **noto** sono conosciuti i risultati per tutte le azioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d997b",
   "metadata": {},
   "source": [
    "![image.png](images/environments_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbe325",
   "metadata": {},
   "source": [
    "### Rappresentazione degli stati e transizioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fc75d",
   "metadata": {},
   "source": [
    "**rappresentazione atomica**: ogni stato del mondo è indivisibile, non ha struttura interna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7a88f",
   "metadata": {},
   "source": [
    "![image.png](images/atomic_state.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a8fe3",
   "metadata": {},
   "source": [
    "**rappresentazione fattorizzata**: suddivide ogni stato in un insieme fissato di variabili o attributi, che possono essere booleani, numeri reali, ...\n",
    "\n",
    "Si può rappresentare l'incertezza lasciando vuoto l'attributo corrispondente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bed285",
   "metadata": {},
   "source": [
    "![image.png](images/factored_state.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e281a96",
   "metadata": {},
   "source": [
    "**rappresentazione strutturata**: include oggetti, ognuno dei quali può avere attributi propri oltre a relazioni con altri oggetti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3cc189",
   "metadata": {},
   "source": [
    "![image.png](images/structured_state.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79789bd8",
   "metadata": {},
   "source": [
    "## Risolvere problemi con la ricerca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286447ef",
   "metadata": {},
   "source": [
    "- metodi utilizzabili da:\n",
    "    - **tipo agente**: **basato su obiettivi**\n",
    "    - **rappresentazione**: **atomica**\n",
    "    - **ambiente**:\n",
    "        - **completamente osservabile** (stato iniziale conosciuto)\n",
    "        - **deterministico** (non si possono gestire eventi inaspettati)\n",
    "        - **statico**\n",
    "        - **completamente noto**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835745d9",
   "metadata": {},
   "source": [
    "**algoritmo di ricerca**: prende un problema come input e restituisce una soluzione sotto forma di sequenza di azioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed37d60",
   "metadata": {},
   "source": [
    "- l'agente svolge ciclicamente i seguenti passi:\n",
    "    1. formula un obiettivo\n",
    "    2. formula un problema da risolvere\n",
    "    3. invoca una procedura di ricerca\n",
    "    4. esegue la sequenza di azioni restituita dalla procedura di ricerca, mentre la esegue vengono ignorate le percezioni, poichè le conosce in anticipo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4737381",
   "metadata": {},
   "source": [
    "![image.png](images/simple_problem_solving_agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26649e2a",
   "metadata": {},
   "source": [
    "### Definizione dei problemi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ad044",
   "metadata": {},
   "source": [
    "- un **problema** è definito da:\n",
    "    - **stato iniziale**: in cui si trova l'agente\n",
    "    - **azioni possibili** dell'agente: dato uno stato $s$, $Azioni(s)$ restituisce l'insieme di azioni che possono essere eseguite in $s$\n",
    "    - **modello di transizione**: descrizione di ciò che ogni azione fa. $Risultato(s, \\alpha)$ restituisce lo stato risultante dall'esecuzione dell'azione $\\alpha$ nello stato $s$\n",
    "    - **test obiettivo**: funzione che verifica se uno stato è l'obiettivo\n",
    "    - **costo cammino**: assegna un costo numerico ad ogni cammino. Costo di passo $c(s, \\alpha, s')$: costo che dell'azione $\\alpha$ che fa passare lo stato $s$ ad $s'$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95871b72",
   "metadata": {},
   "source": [
    "**soluzione**: è una sequenza di azioni che porta dallo stato iniziale ad uno stato obiettivo. La sua qualità è in funzione del costo di cammino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82f4c7",
   "metadata": {},
   "source": [
    "### Cercare soluzioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b605c5",
   "metadata": {},
   "source": [
    "**spazio degli stati**: l'insieme di tutti gli stati raggiungibili a partire da quello iniziale mediante qualsiasi sequenza di azioni.\n",
    "\n",
    "Questo forma un grafo (albero di ricerca), i cui nodi rappresentano gli stati e gli archi le azioni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d750e2",
   "metadata": {},
   "source": [
    "**cammino**: sequenza di stati collegati da una sequenza di azioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e050ec8",
   "metadata": {},
   "source": [
    "**frontiera**: insieme di nodi che sono stati generati e non ancora espansi.\n",
    "\n",
    "Ogni elemento è un nodo foglia (momentaneamente, potrebbe avere figli)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e1ce15",
   "metadata": {},
   "source": [
    "**stato ripetuto**: causato da un cammino ciclico o ridondante, può causare il fallimento di alcuni algoritmi.\n",
    "\n",
    "Per evitare i cammini ridondanti spesso si usa una lista chiusa in cui si tiene traccia di dove si è passati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6def223",
   "metadata": {},
   "source": [
    "#### Strutture dati per algoritmi di ricerca e prestazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb7ba25",
   "metadata": {},
   "source": [
    "- per ogni nodo dell'albero si hanno:\n",
    "    - **stato**\n",
    "    - **padre**\n",
    "    - **azione**\n",
    "    - **costo di cammino**\n",
    "- i nodi vengono memorizzati in una coda le cui operazioni su di essa sono:\n",
    "    - **isEmpty**\n",
    "    - **push**\n",
    "    - **pop**\n",
    "- la coda può essere **FIFO**, **LIFO** o **con priorità**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a65fe",
   "metadata": {},
   "source": [
    "- le prestazioni di misurano in base a:\n",
    "    - **completezza**: l'algoritmo garantisce di trovare una soluzione\n",
    "    - **ottimalità**: l'algoritmo trova la soluzione ottima (quella con costo di cammino più piccolo)\n",
    "    - **complessità temporale**: tempo necessario per trovare la soluzione\n",
    "    - **complessità spaziale**: quanta memoria necessita per effettuare la ricerca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f10b15",
   "metadata": {},
   "source": [
    "- la complessità si esprime usando 3 quantità:\n",
    "    - **fattore di ramificazione $b$**: numero massimo di successori di un nodo (numero di azioni massime che si possono fare su quel nodo come prossima azione)\n",
    "    - **profondità $d$**: del nodo obiettivo più vicino allo stato iniziale, ovvero il numero di passi lungo il cammino a partire dalla radice\n",
    "    - **lunghezza massima $m$**: dei cammini nello spazio degli stati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d8406",
   "metadata": {},
   "source": [
    "### Strategie di ricerca non informata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7871fec2",
   "metadata": {},
   "source": [
    "- sono strategie che non dispongono di informazioni aggiuntive sugli stati oltre a quella fornita dalla definizione del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3b2bfb",
   "metadata": {},
   "source": [
    "#### Ricerca in ampiezza (Breadth-first search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd2c75",
   "metadata": {},
   "source": [
    "- si espande prima il nodo radice, poi tutti i suoi successori, cioè **espande i nodi meno profondi**\n",
    "- tutti i nodi ad un certo livello devono essere espansi prima che si possa espandere uno dei nodi al livello successivo\n",
    "- **frontiera**: coda **FIFO** (i successori sono inseriti in fondo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed5b0b",
   "metadata": {},
   "source": [
    "- proprietà:\n",
    "    - **completezza**: solo se $b$ è finito\n",
    "    - **ottimalità**: solo se il costo per ogni passo $= 1$\n",
    "    - **complessità temporale**: $O(b^{d+1})$\n",
    "    - **complessità spaziale**: $O(b^{d+1})$. Si mantiene ogni nodo in memoria e questo scaturisce il problema principale di questo algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d35ea7",
   "metadata": {},
   "source": [
    "![image.png](images/breadth_first_search_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6efaf38",
   "metadata": {},
   "source": [
    "![image.png](images/breadth_first_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93e87f",
   "metadata": {},
   "source": [
    "#### Ricerca in profondità (Depth-first search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c5c9b",
   "metadata": {},
   "source": [
    "- espande sempre per primo il nodo più profondo nella frontiera dell'albero di ricerca\n",
    "- è problematica solo se $m$ è molto più grande di $d$, altrimenti è più veloce di **breadth-first search**\n",
    "- **frontiera**: coda **LIFO** in cui viene scelto per l'espansione l'ultimo nodo generato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05666901",
   "metadata": {},
   "source": [
    "- proprietà:\n",
    "    - **completezza**: solo negli spazi di stati finiti\n",
    "    - **ottimalità**: no\n",
    "    - **complessità temporale**: $O(b^m)$\n",
    "    - **complessità spaziale**: $O(bm)$. I nodi espansi vengono cancellati dalla coda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8e1c0",
   "metadata": {},
   "source": [
    "![image.png](images/depth_first_search_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bef3b5",
   "metadata": {},
   "source": [
    "#### Ricerca a profondità limitata (Depth-limited search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720a373",
   "metadata": {},
   "source": [
    "- per evitare il problema degli alberi illimitati si imposta un limite alla profondità $= L$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139e6c0",
   "metadata": {},
   "source": [
    "- proprietà:\n",
    "    - **completezza**: solo se $L \\geq d$\n",
    "    - **ottimalità**: no\n",
    "    - **complessità temporale**: $O(b^L)$\n",
    "    - **complessità spaziale**: $O(bL)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de486c5c",
   "metadata": {},
   "source": [
    "![image.png](images/depth_limited_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c68d2f",
   "metadata": {},
   "source": [
    "#### Ricerca ad approfondimento iterativo (Iterative deepening search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88935009",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e6e23a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "587b0250",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2526d67f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39ccfe97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0518788a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0874624",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ddcee5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "088c0f07",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
